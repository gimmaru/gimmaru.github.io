---
layout: post
title:  "[Designing Machine Learning Systems] 모델 개발과 오프라인 평가"
subtitle:   "부제"
categories: study
tags: mlops
comments: true

---

데이터와 피처 엔지니어링에 들인 노력이 출력(예측)값을 제공하는 시스템으로 결실을 맺는 첫번째 단계

개발할 ML 모델을 선택해야 한다. 작업에 가장 적합한 알고리즘을 선택할 때 유용한 6가지 팁을 살펴본다.

모델 개발은 반복 프로세스이다. 반복이 끝날 때마다 모델 성능을 비교해봐야 한다. 모델을 프로덕션 환경에 배포하기 전에 평가하는 방법을 알아본다.

다양한 유형의 일반적인 ML 알고리즘에 대한 지식이 필요하다. 각각의 작동 방식은 다루지 않으며, 알고리즘을 둘러싸고 있는 기법에 초점을 맞춘다.

([Basic ML Review](https://github.com/chiphuyen/dmls-book/blob/main/basic-ml-review.md)는 ML의 기본 개념을 상기하는데 도움이 된다.)

## 6.1 모델 개발과 훈련

모델을 개발하고 훈련하는 데 필요한 여러 측면을 살펴본다.

### 6.1.1 머신러닝 모델 평가

#### 1. SOTA만 추종하는 함정에 빠지지 않기

모델이 구현 측면에서 충분히 빠르거나 비용이 낮다는 뜻이 아니다.

내가 가진 데이터에서 다른 모델보다 성능이 월등하다는 뜻 또한 아니다.

가장 중요한 것은 해결 방안을 찾아내는 일

SOTA 모델보다 훨씬 저렴하고 문제를 단순하게 해결할 방안이 있다면 그것을 사용하는 것이 좋다.

#### 2. 가장 단순한 모델부터 시작하기

단순함은 세 가지 측면에서 유용하다.

- 단순한 모델은 배포하기 쉽다. 모델을 빨리 배포할수록 예측 파이프라인이 훈련 파이프라인과 일치하는지 보다 빨리 확인할 수 있다.

- 단순한 것에서 시작해 복잡한 구성 요소를 단계별로 추가하는 편이 모델을 이해하고 디버깅하기 더 쉽다.

- 가장 단순한 모델은 보다 복잡한 모델의 비교 대상으로서 베이스라인 역할을 수행한다.

#### 3. 모델을 선택할 때 사람의 편향을 주의하기

모델 평가 과정에는 사람의 편향이 들어갈 수밖에 없다.

서로 다른 아키텍처를 비교할 때는 비교 가능한 설정 아래에서 비교하는 것이 중요하다.

#### 4. 현재 성과와 미래 성과를 비교 평가하기

현재 최적인 모델이 두 달 후에도 최적은 아닐 수 있다. 데이터가 적을 때 ML 알고리즘이 나았지만 많아지면 신경망이 더 나을 수 있다.

[학습 곡선](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)으로 데이터가 늘어남에 따라 모델 성능이 어떻게 변할지 가늠해볼 수 있다.

#### 5. 트레이드오프를 평가하기

전형적인 예는 위양성과 위음성의 트레이드오프이다. 지문 잠금 해제 같은 경우 위양성(미승인 사용자를 승인으로 분류하는 경우)을 줄이는 것이 위음성을 줄이는 것보다 중요하다.

연산량 요구 사항과 정확도 간 트레이드오프도 고려해야 한다.

#### 6. 모델의 가정을 이해하기

현실은 엄청나게 복잡하므로 모델은 가정을 통해 어느 정도 근사를 해낼 뿐이다. 모든 단일 모델은 자신만의 가정이 있다. 모델이 무엇을 가정하며 데이터가 그 가정을 실제로 충족하는지 이해한다면 유스 케이스에 가장 적합한 모델이 무엇인지 평가할 수 있다.

흔히 사용하는 가정은 다음과 같다.

- 예측 가정(Prediction assumption)
- IID(Independent and Identically Distributed)
- 매끄러움(Smoothness)
- 계산 가능성(Tractability)
- 경계(Boundaries)
- 조건부 독립(Conditional independence)
- 정규 분포(Normally distributed)

### 6.1.2 앙상블

단일 모델을 개발한 뒤에는 성능을 지속적으로 향상하기 위한 방법을 고민하게 된다. 그 중 한 가지 방법은 앙상블이다.

앙상블은 배포가 복잡하고 유지 관리가 어려워 프로덕션 환경에서는 선호되지 않지만 광고 클릭률 예측처럼 성능이 조금만 향상돼도 금전적 이득이 큰 경우 자주 사용되기도 한다.

앙상블의 분류기 간 상관관계가 없을수록 앙상블의 효과가 크다.

#### 배깅

Bootstrap Aggregating의 줄임말, 분산을 줄이고 과적합을 방지하는 데 효과적이다.

배깅은 일반적으로 불안정성이 높은 기법의 성능을 개선한다.

#### 부스팅

부스팅은 약한 학습기를 강한 학습기로 바꾸는 반복 학습 앙상블 알고리즘의 일종이다.

#### 스태킹

훈련 데이터로 기본 학습기를 훈련하고, 기본 학습기의 출력을 결합해 최종 예측을 수행하는 메타 학습기를 만든다.

앙상블 생성 방법을 보다 자세히 학습하려면 캐글의 전설적인 팀인 [MLWave의 앙상블 가이드 문서](https://github.com/MLWave/Kaggle-Ensemble-Guide?tab=readme-ov-file)를 참조하면 좋다.

### 6.1.3 실험 추적과 버전 관리

실험 진행 상황과 결과를 추적하는 과정을 실험 추적이라고 하며, 나중에 재현되거나 다른 실험과 비교할 목적으로 실험의 모든 세부 정보를 기록하는 프로세스를 버전 관리라고 한다.

아티팩트는 실험 중에 생성되는 파일을 말한다. 아티팩트의 예로 손실 곡선, 평가 손실 그래프, 로그, 훈련 프로세스 전반에 걸쳐 생성되는 모델 중간 결과 등이 있다.

#### 실험 추적

각 실험에 대해 고려할 만한 추적 지표 목록

- 손실 곡선
- 모델 성능 지표
- 샘플, 예측값, 그라운드 트루스 레이블 쌍에 대한 로그
- 모델 훈련 속도
- 시스템 성능 지표
- 매개변수와 하이퍼파라미터

#### 버전 관리

ML 시스템은 일부는 코드이고 일부는 데이터이므로 코드뿐 아니라 데이터도 버전을 지정해야 한다.

데이터는 크기가 코드보다 훨씬 크기 때문에 코드 버전 관리 전략을 데이터 버전 관리에 동일하게 사용하기 어렵다.

##### ML 모델 디버깅

ML 모델 디버깅이 어려운 이유

- ML 모델 실행은 명시적인 중단이나 출력 없이 조용히 실패하곤 한다.
- 버그를 찾았다고 생각하더라도 버그가 수정됐는지 확인하는 작업은 답답할 만큼 느리다.
- ML 모델 디버깅은 기능 간 복잡도로 인해 매우 까다롭다.

ML 모델 실패의 대표적인 원인
- 이론상의 제약 조건
- 잘못된 모델 구현
- 잘못된 하이퍼파라미터 선택
- 데이터 문제
- 잘못된 피처 선택

ML 모델 디버깅을 위한 몇 가지 방법들

- 단순하게 시작하고 점진적으로 구성 요소를 추가하기
- 단일 배치에 과적합시키기
- 무작위 시드 값을 고정하기

### 6.1.4 분산 훈련

대규모 ML을 수행할 때 발생 가능한 문제점에 집중한다.

#### 데이터 병렬 처리

여러 머신에 데이터를 분할하고, 각 머신에서 모델을 훈련하고, 그래디언트를 합산한다.

#### 모델 병렬 처리

모델의 각기 다른 구성 요소를 서로 다른 머신에서 훈련한다.

### 6.1.5 오토ML

실제 문제를 풀기 위해 ML 알고리즘 탐색 프로세스를 자동화하는 것을 말한다.

#### 소프트 오토ML: 하이퍼파리미터 조정

하이퍼파리미터 조정에 대한 체계적인 접근법

auto-sklearn, Keras Tuner, Tune 등

무작위 탐색, 격자 탐색, 베이즈 최적화가 인기 있다.

#### 하드 오토ML: 아키텍처 탐색과 학습된 옵티마이저

모델의 서로 다른 구성 요소 혹은 모델 전체를 하이퍼파라미터로 간주하면?

관련 연구 분야를 NAS(Neural Architecture Search)라고 하고, 최적의 신경망 모델 아키텍처를 탐색한다는 의미이다.

3가지 구성 요소

- 탐색 공간
- 성능 추정 전략
- 탐색 전략


옵티마이저는 하이퍼파리미터 설정에 민감하고 기본 하이퍼파라미터가 아키텍처 후보 전체에서 두루 잘 동작하지 않으므로 실제로 해당 방법을 적용하기는 어렵다. -> 업데이트 규칙을 정하는 함수를 신경망으로 대체하면 어떨까?

옵티마이저를 신경망으로 구성하고, 해당 옵티마이저를 학습시키는 방법이 등장했다.


## 6.2 모델 오프라인 평가

모델 배포 전에 성능을 평가하는 방법을 알아본다.

### 6.2.1 베이스라인

평가 지표는 그 자체로는 큰 의미가 없다. 모델을 평가할 때는 평가 기준을 명확히 알아야 한다.

정확한 베이스라인은 유스 케이스마다 다르지만 대부분의 경우에 유용한 베이스라인은 다음과 같다.

- 무작위 베이스라인

- 단순 휴리스틱

- 0 규칙 베이스라인

- 인간에 의한 베이스라인

- 기존 솔루션

모델을 평가할 때 '좋은 시스템'과 '유용한 시스템'을 구별하는 것이 중요하다.

### 6.2.2 평가 방법

프로덕션 환경에서는 모델이 강건하고, 공정하고, 잘 보정되고, 개연성 있는 결과를 내놓기를 원한다.

#### 교란 테스트

모델 개발에 사용하는 입력값이 프로덕션 환경에서 다뤄야 하는 입력값과 유사해야 하지만 이는 대부분 현실적으로 어렵다.

데이터에 잡음이 있을 때 모델이 얼마나 잘 작동하는지 알아보는 방법으로, 테스트 분할에 약간 변화를 줘서 모델 성능에 어떤 영향을 미치는지 측정해봐도 좋다.

#### 불변성 테스트

입력에 특정 변경을 적용했을 때 출력이 변해서는 안 된다.

나머지 입력은 동일하게 유지하고 민감 정보만 변경해 출력이 바뀌는지 확인해보면 좋다. 더 좋은 방법은 모델 훈련에 사용한 피처에서 민감 정보를 최우선으로 제거하는 것이다.

#### 방향 예상 테스트

입력에 특정 변경을 적용했을 때 출력이 예측 가능한 방향으로 변해야 한다. 출력이 예상과 반대로 움직인다면 모델 학습이 올바르게 이뤄지지 못했을 수 있으므로 배포 전에 좀 더 조사해봐야 한다.

#### 모델 보정

모델이 A팀이 B팀을 70% 확률로 이길 거라고 예측했는데 1,000회 대결 중 A팀이 60%만 이긴다면 이 모델은 보정되지 않은 것이다. 잘 보정된 모델은 A팀이 60% 확률로 승리할 거라고 예측해야 한다.

일반적인 모델 보정 방법은 플랫 스케일링이며 사이킷런에서는 [sklearn.calibration.CalibratedClassifierCV](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html)로 구현한다.

더 자세히 알아보고 싶다면 [모델 보정이 중요한 이유와 이를 수행하는 방법](https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html)을 읽어보기 바란다.

#### 신뢰도 측정

신뢰도 측정은 개별 예측의 유용성에 대한 임곗값을 생각해보는 방식이다.

#### 슬라이스 기반 평가

슬라이싱은 데이터를 하위 집합으로 분리하고 각 하위 집합마다 모델의 개별 성능을 확인하는 것이다.

슬라이스 기반 평가가 중요한 이유는 흥미롭지만 직관에 반하는 심슨의 역설 때문이다.

요점은 집계가 실제 상황을 은폐하고 모순되게 할 수 있다는 점이다.

데이터에서 중요한 슬라이스를 찾는 주요 접근법 세 가지

- 휴리스틱 기반
- 오류 분석
- 슬라이스 파인더

## 6.3 정리

가장 먼저, 작업에 가장 적합한 ML 모델을 선택하는 방법을 알아보았다. 어떤 모델이 특정 목표, 제약 조건, 요구 사항에 가장 적합한지를 감안해 의사 결정을 하기 위해 고려할 측면을 살펴보았다.

모델 개발의 다양한 측면을 다뤘다.

데이터 병렬, 모델 병렬을 비롯한 병렬 기술 전반을 논의했다.

배포에 가장 적합한 모델을 선택하기 위해 모델을 평가하는 방법을 다뤘다.